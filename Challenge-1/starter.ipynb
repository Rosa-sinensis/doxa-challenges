{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCLAIS Tutorial Series Challenge 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are proud to present you with the first challenge of the 2022-23 UCLAIS tutorial series: brain stroke prediction. You will be introduced to a variety of core concepts in machine learning and their implementation using `scikit-learn`. \n",
    "\n",
    "This Jupyter notebook will guide you through the various general stages involved in end-to-end machine learning projects, including data visualisation, data preprocessing, model selection, model training and model evaluation. Finally, you will get the chance to submit your results to [DOXA](https://doxaai.com/).\n",
    "\n",
    "If you do not already have a DOXA account, you will want to [sign up](https://doxaai.com/sign-up) first before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background & Motivation\n",
    "\n",
    "![title](https://www.cdc.gov/stroke/images/Stroke-Medical-Illustration.jpg?_=77303?noicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**: A stroke occurs when the bloody supply to the brain is blocked or when a blood vessel within the brain bursts. Brain cells can begin to die within minutes, so a stroke is a medical emergency, and prompt treatment is crucial. A stroke can cause lasting brain damage, long-term disability and eventually death, so early action is crucial to minimise brain damage and other complications.\n",
    "\n",
    "**Objective**: Our objective is to be able to predict whether a person has a stroke or not given some information about them.\n",
    "\n",
    "**Dataset**: The dataset is based on the following [stroke prediction dataset](https://www.kaggle.com/datasets/zzettrkalpakbal/full-filled-brain-stroke-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/max/1400/0*V0GyOt3LoDVfY7y5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you already know, the machine learning process covers a wide set of steps. As you go through this notebook, try to keep in mind which stage are we dealing with at that moment and what we are trying to achieve. \n",
    "\n",
    "As you reach the end of the notebook, you will notice that the sixth step (parameter tuning) from the figure above is missing; this is a challenge for you! Be creative and try to learn something new as you implement your ideas. \n",
    "\n",
    "There are a lot of helpful resources online you can use, such as the excellent `scikit-learn` [documentation](https://scikit-learn.org/stable/getting_started.html). This will hopefully allow you to improve your score on the DOXA scoreboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Useful Packages\n",
    "\n",
    "To get started, we will install a number of common machine learning packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U numpy pandas matplotlib seaborn scikit-learn doxa-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import relevant sklearn classes/functions related to data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OrdinalEncoder\n",
    "\n",
    "# Import relevant sklearn classes related to machine learning models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso\n",
    "from sklearn.svm import SVC, SVR, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "\n",
    "# Import relevant sklearn class/function related to evaluation\n",
    "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay         \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training dataset\n",
    "data_original = pd.read_csv(\"./data/train.csv\")  # Change the path accordingly\n",
    "\n",
    "# We then make a deep copy of the dataset that we can manipulate\n",
    "# and process while leaving the original intact\n",
    "data = data_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding & Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to train our Machine Learning model, it is important to have a look and understand first the dataset that we will be using. This will provide some insights onto which model, model hyperparameter, and loss function are suitable for the problem we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the first 15 entries of our dataset\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the size and shape of our training data\n",
    "print(f\"Shape: {data.shape}\\n\")\n",
    "\n",
    "# Display the list of features we have\n",
    "print(f\"List of features: {data.columns}\\n\")\n",
    "\n",
    "# Check for any missing values\n",
    "print(\"Missing values: \")\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataframe and simple analysis above, there are several things we can observe:\n",
    "\n",
    "- There are 10 features (excluding `stroke`, which we are trying to predict) and 4500 samples\n",
    "- The features in our dataset involve both numerical and categorical value\n",
    "- The range of the numerical features in our dataset varies significantly\n",
    "- There are no missing data values in our dataset\n",
    "- We are dealing with a binary classification problem, where the output is either 0 or 1\n",
    "\n",
    "One of the most important findings from listing even just the first 15 values of our dataset is that we are dealing with an imbalanced classification problem, where the output (whether a person has had a stroke or not) is heavily skewed towards 0 (i.e. not having had a stroke)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stroke_false = len(data[data[\"stroke\"] == 0])\n",
    "num_stroke_true = len(data[data[\"stroke\"] == 1])\n",
    "\n",
    "print(f\"Number of people that have stroke: {num_stroke_false}\")\n",
    "print(f\"Number of people that have stroke: {num_stroke_true}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, almost 95% of our sample has a label of 0, which indicates that most people in our dataset have not had a stroke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation\n",
    "\n",
    "In general, we know that as the age of a person increases, the chance of that person having a stroke also increases. \n",
    "\n",
    "But, is this true? And does it apply to this dataset? We can verify this correlation by producing a plot of the rate of having a stroke against age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax0 = fig.add_subplot()\n",
    "\n",
    "data[\"age\"] = data[\"age\"].astype(int)\n",
    "\n",
    "# Calculate the rate of a person getting stroke as a function of age\n",
    "rate = []\n",
    "for i in range(data[\"age\"].min(), data[\"age\"].max()):\n",
    "    rate.append(\n",
    "        data[data[\"age\"] < i][\"stroke\"].sum() / len(data[data[\"age\"] < i][\"stroke\"])\n",
    "    )\n",
    "\n",
    "# Draw a lineplote\n",
    "sns.lineplot(data=rate, ax=ax0)\n",
    "\n",
    "# Remove the top, right, and left surrounding line for aesthetic purposes\n",
    "for s in [\"top\", \"right\", \"left\"]:\n",
    "    ax0.spines[s].set_visible(False)\n",
    "\n",
    "# Adjust the tick appearance for aesthetic purposes\n",
    "ax0.tick_params(axis=\"both\", which=\"major\", labelsize=8)\n",
    "ax0.tick_params(axis=\"both\", which=\"both\", length=0)\n",
    "\n",
    "# Add some text on the figure\n",
    "ax0.text(\n",
    "    -3,\n",
    "    0.055,\n",
    "    \"Stroke Risk by Age\",\n",
    "    fontsize=18,\n",
    "    fontfamily=\"serif\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax0.text(\n",
    "    -3,\n",
    "    0.047,\n",
    "    \"As people age, the risk of having a stroke increases\",\n",
    "    fontsize=14,\n",
    "    fontfamily=\"serif\",\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep - as expected, the higher the age, the higher the chance of having a stroke. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few more things we need to do before we can start training a machine learning model. Among them are the following:\n",
    "- Converting categorical data into numerical data\n",
    "- Standardising the range of our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find categorical features, along with their values\n",
    "# We do this by exploiting the fact that categorical features have a data type of 'object'\n",
    "\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == \"object\":\n",
    "        print(col, data[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical values\n",
    "data[\"gender\"] = data[\"gender\"].replace({\"Male\": 0, \"Female\": 1}).astype(np.uint8)\n",
    "data[\"ever_married\"] = (\n",
    "    data[\"ever_married\"].replace({\"No\": 0, \"Yes\": 1}).astype(np.uint8)\n",
    ")\n",
    "data[\"work_type\"] = (\n",
    "    data[\"work_type\"]\n",
    "    .replace(\n",
    "        {\n",
    "            \"Private\": 0,\n",
    "            \"Self-employed\": 1,\n",
    "            \"Govt_job\": 2,\n",
    "            \"children\": 3,\n",
    "            \"Never_worked\": 4,\n",
    "        }\n",
    "    )\n",
    "    .astype(np.uint8)\n",
    ")\n",
    "data[\"Residence_type\"] = (\n",
    "    data[\"Residence_type\"].replace({\"Rural\": 0, \"Urban\": 1}).astype(np.uint8)\n",
    ")\n",
    "data[\"smoking_status\"] = (\n",
    "    data[\"smoking_status\"]\n",
    "    .replace({\"formerly smoked\": 0, \"smokes\": 1, \"never smoked\": 2, \"Unknown\": 3})\n",
    "    .astype(np.uint8)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check our dataset to see whether our categorical data has correctly been changed into numerical data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now, we have converted the categorical features in our dataset into numerical features. A faster way of doing this is by using the [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) class provided by scikit-learn.\n",
    "\n",
    "**Challenge**: have a think about the way we have encoded our categorical data - what are the potential consequences of encoding categorical data in this way? What might be a better way of encoding this type of data? We'll discuss this further later on in the notebook.\n",
    "\n",
    "Next, let's standardise the numerical features by taking off the mean and scaling the data to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data[[\"age\", \"avg_glucose_level\", \"bmi\"]] = scaler.fit_transform(\n",
    "    data[[\"age\", \"avg_glucose_level\", \"bmi\"]]\n",
    ")\n",
    "\n",
    "# Verify that our feature has been standardized\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data has been standardised and all our features are numerical, we are very close to training our first machine learning model.\n",
    "\n",
    "All that is left to do is the following:\n",
    "\n",
    "1. **Separate the input features and the output label**: this is an important requirement when training our dataset - we don't want to train our `scikit-learn` models on the data we are trying to predict!\n",
    "\n",
    "2. **Split our data into training and test sets**: the training set is the dataset on which our models will be trained. After training our models, we then test them on our newly created test set.\n",
    "\n",
    "We will use the **empirical error** from evaluating our models on the test set as a proxy for the **generalisation error**: a measure of how accurately an algorithm can predict outcomes for unseen data (which is what we are trying to do eventually!). It will also provide us with a useful tool for comparing the different models we have trained so that we can decide which model to use for our submission to DOXA. Bam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate our data into X, which contains all the features in our dataset and y, which contains only the output/label (stroke)\n",
    "X = data.drop(columns=['stroke'])\n",
    "y = data['stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that we have correctly separated the features and the output by looking at the shape of X and y\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our features and output into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# In this case, the test_size parameter is equal to 0.2, so our test set will \n",
    "# have 20% the data, while the training set will have the other 80% of the data\n",
    "\n",
    "# TODO: try changing the test_size parameter and see whether it impacts the performance of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the operation ran as intended by checking the shape of the splitted dataset\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the magic begins. As an example, we will be training our dataset by using three different models and choosing the best model for submission later. The models we will be testing out are [logistic regression models](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), [support vector machines](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) and [decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). \n",
    "\n",
    "Bear in mind that each of the different types of model has its own set of hyperparameters that you can tune to improve performance. Do check out the documentation for each type of model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X=X_train, y=y_train)\n",
    "\n",
    "clf_svm = SVC()\n",
    "clf_svm.fit(X=X_train, y=y_train)\n",
    "\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "clf_tree.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "Now that we have trained our machine learning models, we can test them on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .predict() method to predict output values for our test set\n",
    "lr_predicted = clf_lr.predict(X_test)\n",
    "svm_predicted = clf_svm.predict(X_test)\n",
    "tree_predicted = clf_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the accuracy_score() function as our evaluation metric, which simply calculates \n",
    "# the number of predictions that are correct and divides it by the total number of predictions.\n",
    "lr_accuracy = accuracy_score(lr_predicted, y_test)\n",
    "svm_accuracy = accuracy_score(svm_predicted, y_test)\n",
    "tree_accuracy = accuracy_score(tree_predicted, y_test)\n",
    "\n",
    "print(\"Accuracy (Logistic Regression): \", lr_accuracy)\n",
    "print(\"Accuracy (SVM): \", svm_accuracy)\n",
    "print(\"Accuracy (Decision Tree): \", tree_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! We can see that the logistic regression model and the SVM performed equally well (with about 95% accuracy), while our decision tree has slightly worst performance.\n",
    "\n",
    "Let's put things back into perspective. Right now, we are doing an imbalanced classification problem for which only 5% of our outputs have a value of 1; thus, we could easily achieve 95% accuracy just by always outputting 0 even! There is definitely a long way to go with regards to producing a model that is able to predict accurately whether a person has a stroke or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do further analysis by confusion matrices\n",
    "print(\"Confusion Matrix (Logistic Regression)\")\n",
    "print(ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=lr_predicted))\n",
    "\n",
    "print(\"\\nConfusion Matrix (SVM)\")\n",
    "print(ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=svm_predicted))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Decision Tree)\")\n",
    "print(ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=tree_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our DOXA Submission\n",
    "\n",
    "Once we are confident with the performance of our model, we can start deploying it on the real test dataset for submission to DOXA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import our test dataset and save it in a variable called data_test\n",
    "data_test = pd.read_csv(\"./data/test.csv\")          # Change the path accordingly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we must preprocess the dataset before feeding it into the trained model. The preprocessing steps include: \n",
    "1. Converting categorical data into numerical data\n",
    "2. Standardising numerical data that has a large range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical values as we did before\n",
    "data_test[\"gender\"] = (\n",
    "    data_test[\"gender\"].replace({\"Male\": 0, \"Female\": 1}).astype(np.uint8)\n",
    ")\n",
    "data_test[\"ever_married\"] = (\n",
    "    data_test[\"ever_married\"].replace({\"No\": 0, \"Yes\": 1}).astype(np.uint8)\n",
    ")\n",
    "data_test[\"work_type\"] = (\n",
    "    data_test[\"work_type\"]\n",
    "    .replace(\n",
    "        {\n",
    "            \"Private\": 0,\n",
    "            \"Self-employed\": 1,\n",
    "            \"Govt_job\": 2,\n",
    "            \"children\": 3,\n",
    "            \"Never_worked\": 4,\n",
    "        }\n",
    "    )\n",
    "    .astype(np.uint8)\n",
    ")\n",
    "data_test[\"Residence_type\"] = (\n",
    "    data_test[\"Residence_type\"].replace({\"Rural\": 0, \"Urban\": 1}).astype(np.uint8)\n",
    ")\n",
    "data_test[\"smoking_status\"] = (\n",
    "    data_test[\"smoking_status\"]\n",
    "    .replace({\"formerly smoked\": 0, \"smokes\": 1, \"never smoked\": 2, \"Unknown\": 3})\n",
    "    .astype(np.uint8)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise numerical features\n",
    "scaler = StandardScaler()\n",
    "data_test[[\"age\", \"avg_glucose_level\", \"bmi\"]] = scaler.fit_transform(\n",
    "    data_test[[\"age\", \"avg_glucose_level\", \"bmi\"]]\n",
    ")\n",
    "\n",
    "# Output the shape of our submission dataset\n",
    "print(f\"Shape: {data_test.shape}\")\n",
    "\n",
    "# Verify that our features have been standardised\n",
    "data_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have redone all the preprocessing stages, we can proceed to do inference on the DOXA submission test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will choose the logistic regression model\n",
    "predictions = clf_lr.predict(data_test)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the output is the shape it should be, having 481 entries, so we are now ready to submit our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"submission\", exist_ok=True)\n",
    "\n",
    "with open(\"submission/y.txt\", \"w\") as f:\n",
    "    f.writelines([f\"{prediction}\\n\" for prediction in predictions])\n",
    "\n",
    "with open(\"submission/doxa.yaml\", \"w\") as f:\n",
    "    f.write(\"competition: uclais-1\\nenvironment: cpu\\nlanguage: python\\nentrypoint: run.py\")\n",
    "\n",
    "with open(\"submission/run.py\", \"w\") as f:\n",
    "    f.write(\"with open('y.txt', 'r') as f: print(f.read().strip())\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to DOXA\n",
    "\n",
    "Before you can submit to DOXA, you must first ensure that you are enrolled for the challenge on the DOXA website. Visit [the challenge page](https://doxaai.com/competition/uclais-1) and click \"Enrol\" in the top-right corner.\n",
    "\n",
    "You can then log in using the DOXA CLI by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then submit your results to DOXA by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa upload submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! You have (probably) just uploaded your first submission to DOXA! Take a moment to see where you are on the [scoreboard](https://doxaai.com/competition/uclais-1)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is not that good at predicting stroke since it mainly just outputs 0, so there is definitely scope for improvement! Here are a few ways we could improve the process:\n",
    "\n",
    "**1. Data Visualisation**\n",
    "- Visualise other features as well (rather than just age) to see what other features correlate with a person having a stroke. We could potentially produce a correlation matrix.\n",
    "\n",
    "**2. Data Preprocessing**\n",
    "- Apply the [PCA algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), which will reduce the number of input features to a smaller subset that matters more to us. We choose `n` input features that have the highest orthogonality, where `n` is a hyperparameter, so tuning needs to be done to get the best performance.\n",
    "\n",
    "- Perhaps an ordinal encoding is not the most appropriate for our categorical data. We could, for example, try using a [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) instead!\n",
    "\n",
    "**3. Dealing with Imbalanced Dataset**\n",
    "- The challenge of working with imbalanced datasets is that many ML models will just ignore the minority class (as you can see if you run through a decision matrix for the SVM and logistic regression models from earlier).\n",
    "- One approach to address this is to oversample the minority class. The simplest approach involves duplicating examples in the minority class, although these will not create any new meaningful information for the model. Instead, new examples can be synthesised from the existing examples. This type of data augmentation is referred to as the [Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/) (SMOTE).\n",
    "\n",
    "**4. Model Selection**\n",
    "- In our example, we have looked at implementing [logistic regression models](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression), [support vector machines](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) and [decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Each of these models has its own set of hyperparameters that you can tune to improve model performance. The link will bring you to the documentation page of `scikit-learn` where you can discover more about the hyperparameters of each type of model.\n",
    "- On top of that, there are many more machine learning model types that you can try out and see whether accuracy improves or not. Indeed, there are even ensemble methods that use multiple machine learning models under the hood! \n",
    "- If you look at the different machine learning models being imported at the start of the notebook, you will notice that there are quite a few which have not been used. This might be a good starting point!\n",
    "\n",
    "And perhaps, many more..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
